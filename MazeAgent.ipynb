{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ee42c5a9-9a1e-448a-86af-aa5d48492b10",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-06T07:06:08.200553Z",
     "iopub.status.busy": "2025-10-06T07:06:08.200553Z",
     "iopub.status.idle": "2025-10-06T07:06:08.217308Z",
     "shell.execute_reply": "2025-10-06T07:06:08.216257Z",
     "shell.execute_reply.started": "2025-10-06T07:06:08.200553Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "from collections import deque\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6511ef9e-26cf-4bfc-92fd-0a16e2dc2e87",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-06T07:06:08.507270Z",
     "iopub.status.busy": "2025-10-06T07:06:08.507270Z",
     "iopub.status.idle": "2025-10-06T07:06:08.545358Z",
     "shell.execute_reply": "2025-10-06T07:06:08.545358Z",
     "shell.execute_reply.started": "2025-10-06T07:06:08.507270Z"
    }
   },
   "outputs": [],
   "source": [
    "class MultiAgentMazeEnv:\n",
    "    def __init__(self, size=(5, 5), starts=[(0, 0), (0, 4)], goals=[(5, 5), (5, 5)], walls=None):\n",
    "        self.size = size\n",
    "        self.starts = starts\n",
    "        self.goals = goals\n",
    "        self.walls = walls if walls else []\n",
    "        self.n_agents = len(starts)\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.agent_positions = [list(start) for start in self.starts]\n",
    "        return [tuple(pos) for pos in self.agent_positions]\n",
    "\n",
    "    def step(self, actions, weights=None):\n",
    "        \"\"\"\n",
    "        actions: list of integers (0=up, 1=down, 2=left, 3=right)\n",
    "        returns: new positions, rewards, dones\n",
    "        \"\"\"\n",
    "        moves = {\n",
    "            0: (-1, 0),  # up\n",
    "            1: (1, 0),   # down\n",
    "            2: (0, -1),  # left\n",
    "            3: (0, 1),   # right\n",
    "        }\n",
    "\n",
    "        n = len(actions)\n",
    "        order = list(range(n))  # default order\n",
    "        \n",
    "        if weights is None:\n",
    "            weights = np.ones(n, dtype=float)  # equal weights\n",
    "        else:\n",
    "            weights = np.array(weights, dtype=float)\n",
    "            \n",
    "    \n",
    "        if weights is not None:\n",
    "            # --- Weighted random order (no replacement) ---\n",
    "            order = []\n",
    "            remaining_agents = list(range(n))\n",
    "            remaining_weights = np.array(weights, dtype=float).copy()\n",
    "    \n",
    "            while remaining_agents:\n",
    "                probs = remaining_weights / remaining_weights.sum()\n",
    "                idx = np.random.choice(len(remaining_agents), p=probs)\n",
    "                order.append(remaining_agents.pop(idx))\n",
    "                remaining_weights = np.delete(remaining_weights, idx)\n",
    "\n",
    "        new_positions = []\n",
    "        for i in range(0,n): \n",
    "            new_positions.append(tuple(map(int,[-1,-1])))\n",
    "\n",
    "        agent_pos_temp = np.array(self.agent_positions, dtype=float).copy()\n",
    "        for i in order:   # process agents in weighted-random order\n",
    "            action = actions[i]\n",
    "            move = moves[action]\n",
    "            new_pos = [agent_pos_temp[i][0] + move[0],\n",
    "                       agent_pos_temp[i][1] + move[1]]\n",
    "\n",
    "            # check bounds and walls\n",
    "            if (0 <= new_pos[0] < self.size[0] and 0 <= new_pos[1] < self.size[1] and tuple(new_pos) not in self.walls):\n",
    "                if tuple(new_pos) not in new_positions:\n",
    "                    agent_pos_temp[i] = new_pos  # update if valid\n",
    "                    new_positions[i] = tuple(map(int,agent_pos_temp[i]))\n",
    "                elif tuple(new_pos) in new_positions:\n",
    "                    new_positions[i] = tuple(map(int,self.agent_positions[i]))\n",
    "            else:\n",
    "                if tuple(agent_pos_temp[i]) in new_positions:\n",
    "                    new_positions = list(self.agent_positions)\n",
    "                    break\n",
    "                else:\n",
    "                    new_positions[i] = tuple(map(int,self.agent_positions[i]))\n",
    "        self.agent_positions = new_positions\n",
    "        \n",
    "        rewards = np.zeros(n, dtype=float)\n",
    "        dones   = np.zeros(n, dtype=bool)  \n",
    "        # who reached their goal this step?\n",
    "        reached = [i for i in range(n) if tuple(new_positions[i]) == self.goals[i]]\n",
    "        \n",
    "        if len(reached) == 1:\n",
    "            w = reached[0]\n",
    "            rewards[w] = 1.0\n",
    "            for j in range(n):\n",
    "                if j != w:\n",
    "                    rewards[j] = -1.0\n",
    "            dones[:] = True  # end episode after a win\n",
    "        elif len(reached) >= 2:\n",
    "            # simultaneous arrival: tie â†’ zeros, end episode\n",
    "            rewards[:] = 0.0\n",
    "            dones[:] = True\n",
    "        else:\n",
    "            # no one reached: keep going\n",
    "            rewards[:] = 0.0\n",
    "            dones[:] = False\n",
    "        \n",
    "        return new_positions, rewards, dones\n",
    "\n",
    "    def render(self):\n",
    "        grid = np.full(self.size, \" \", dtype=object)\n",
    "    \n",
    "        # Draw walls\n",
    "        for w in self.walls:\n",
    "            grid[w] = \"#\"\n",
    "    \n",
    "        # Draw numbered goals (colored numbers)\n",
    "        colors = [\"\\033[92m\", \"\\033[94m\", \"\\033[91m\", \"\\033[93m\"]  # green, blue, red, yellow\n",
    "        for i, g in enumerate(self.goals):\n",
    "            color = colors[i % len(colors)]\n",
    "            grid[g] = f\"{color}{i}\\033[0m\"\n",
    "    \n",
    "        # Draw numbered agents\n",
    "        for i, pos in enumerate(self.agent_positions):\n",
    "            grid[tuple(pos)] = f\"{i}\"\n",
    "    \n",
    "        # Print\n",
    "        for row in grid:\n",
    "            print(\" \".join(row))\n",
    "        print(\"----------\")\n",
    "\n",
    "\n",
    "\n",
    "def random_maze_env(size=(5,5), n_agents=1, n_walls=6, seed=None):\n",
    "    \"\"\"\n",
    "    Build a random MultiAgentMazeEnv with given size, agents, and wall count.\n",
    "    \n",
    "    Args:\n",
    "        size (tuple): grid size (rows, cols)\n",
    "        n_agents (int): number of agents\n",
    "        n_walls (int): number of wall cells\n",
    "        seed (int or None): random seed for reproducibility\n",
    "    \n",
    "    Returns:\n",
    "        env: a MultiAgentMazeEnv instance\n",
    "    \"\"\"\n",
    "    if seed is not None:\n",
    "        random.seed(seed)\n",
    "    else:\n",
    "        seed = random.randint(0, int(1e9))\n",
    "        random.seed(seed)\n",
    "        print(\"Generated seed:\", seed)\n",
    "\n",
    "    rows, cols = size\n",
    "\n",
    "    # --- choose starts and goals ---\n",
    "    all_cells = [(r, c) for r in range(rows) for c in range(cols)]\n",
    "    starts = random.sample(all_cells, n_agents)\n",
    "    remaining = [c for c in all_cells if c not in starts]\n",
    "    goals = random.sample(remaining, n_agents)\n",
    "\n",
    "    # --- choose walls (avoid starts + goals) ---\n",
    "    forbidden = set(starts + goals)\n",
    "    candidates = [c for c in all_cells if c not in forbidden]\n",
    "    walls = random.sample(candidates, min(n_walls, len(candidates)))\n",
    "\n",
    "    # --- build env ---\n",
    "    env = MultiAgentMazeEnv(\n",
    "        size=size,\n",
    "        starts=starts,\n",
    "        goals=goals,\n",
    "        walls=walls\n",
    "    )\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "51d08a71-05ec-4f8d-b2dd-6e57fbac5c1c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-06T09:28:09.864865Z",
     "iopub.status.busy": "2025-10-06T09:28:09.863865Z",
     "iopub.status.idle": "2025-10-06T09:28:09.879494Z",
     "shell.execute_reply": "2025-10-06T09:28:09.878497Z",
     "shell.execute_reply.started": "2025-10-06T09:28:09.864865Z"
    }
   },
   "outputs": [],
   "source": [
    "def solution(env, scores, policies):\n",
    "    \"\"\"\n",
    "    Combined solution that works with both LinearPolicy and neural network policies\n",
    "    \"\"\"\n",
    "    \n",
    "    # Optional: Plot training progress\n",
    "    # plot_training_progress(scores)\n",
    "    \n",
    "    # --- Run one greedy rollout ---\n",
    "    dones = [False] * env.n_agents\n",
    "    states = env.reset()\n",
    "    env.render()\n",
    "    \n",
    "    max_steps = 20\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        actions = []\n",
    "        for i in range(env.n_agents):\n",
    "            # Detect policy type and get greedy action accordingly\n",
    "            if hasattr(policies[i], '_phi'):\n",
    "                # LinearPolicy case - use _phi method\n",
    "                phi = policies[i]._phi(states[i])\n",
    "                logits = phi @ policies[i].W\n",
    "                probs = torch.softmax(logits, dim=0)\n",
    "                action = torch.argmax(probs, dim=0).item()\n",
    "            else:\n",
    "                # Neural network policy case - call directly\n",
    "                state_tensor = torch.tensor(states[i], dtype=torch.float32).unsqueeze(0)\n",
    "                probs = policies[i](state_tensor)\n",
    "                action = torch.argmax(probs, dim=1).item()\n",
    "            \n",
    "            actions.append(action)\n",
    "\n",
    "        next_states, step_rewards, dones = env.step(actions)\n",
    "        states = next_states\n",
    "        env.render()\n",
    "        if any(dones):\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "101986ff-8bb9-4171-a5aa-58cce52bbc8c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-06T09:28:10.757005Z",
     "iopub.status.busy": "2025-10-06T09:28:10.757005Z",
     "iopub.status.idle": "2025-10-06T09:28:10.779060Z",
     "shell.execute_reply": "2025-10-06T09:28:10.778520Z",
     "shell.execute_reply.started": "2025-10-06T09:28:10.757005Z"
    }
   },
   "outputs": [],
   "source": [
    "class PolicyNet(nn.Module): #Â definie the policy network\n",
    "    def __init__(self, state_size=2, action_size=4, hidden_size=32):\n",
    "        super(PolicyNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, action_size)\n",
    "        \n",
    "        # force initialization for equal action probs\n",
    "        nn.init.zeros_(self.fc2.weight)\n",
    "        nn.init.zeros_(self.fc2.bias)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = self.fc2(x)\n",
    "        return F.softmax(x, dim=1) # we just consider 1 dimensional probability of action\n",
    "\n",
    "    def act(self, state):\n",
    "        state = np.array(state, dtype=np.float32)\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(\"cpu\")\n",
    "        probs = self.forward(state).cpu()\n",
    "        model = Categorical(probs)\n",
    "        action = model.sample()\n",
    "        return action.item(), model.log_prob(action), model.entropy()\n",
    "\n",
    "class LinearPolicy:\n",
    "    def __init__(self, state_size=2, action_size=4, maze_size=None):\n",
    "        self.action_size = action_size\n",
    "        self.maze_size = np.array(maze_size if maze_size is not None else [1,1], dtype=np.float32)\n",
    "        self.W = torch.zeros(state_size + 1, action_size, requires_grad=True)\n",
    "\n",
    "    def _phi(self, state):\n",
    "        s = np.array(state, dtype=np.float32) / self.maze_size\n",
    "        phi = np.append(s, 1.0)\n",
    "        return torch.from_numpy(phi).float()\n",
    "\n",
    "    def act(self, state):\n",
    "        phi = self._phi(state)\n",
    "        logits = phi @ self.W\n",
    "        probs = torch.softmax(logits, dim=0)\n",
    "        dist  = Categorical(probs)\n",
    "        a     = dist.sample()\n",
    "        return a.item(), dist.log_prob(a), dist.entropy()\n",
    "        #return a.item(), dist.log_prob(a)\n",
    "    def parameters(self):\n",
    "        return [self.W]\n",
    "\n",
    "\n",
    "class LinearPolicy:\n",
    "    def __init__(self, state_size=2, action_size=4, maze_size=None, epsilon=0.1, epsilon_decay=0.995, min_epsilon=0.1):\n",
    "        self.action_size = action_size\n",
    "        self.maze_size = np.array(maze_size if maze_size is not None else [1,1], dtype=np.float32)\n",
    "        self.W = torch.zeros(state_size + 1, action_size, requires_grad=True)\n",
    "        \n",
    "        # Epsilon-greedy parameters\n",
    "        self.epsilon = epsilon\n",
    "        self.initial_epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.min_epsilon = min_epsilon\n",
    "\n",
    "    def _phi(self, state):\n",
    "        s = np.array(state, dtype=np.float32) / self.maze_size\n",
    "        phi = np.append(s, 1.0)\n",
    "        return torch.from_numpy(phi).float()\n",
    "\n",
    "    def act(self, state, training=True):\n",
    "        phi = self._phi(state)\n",
    "        logits = phi @ self.W\n",
    "        probs = torch.softmax(logits, dim=0)\n",
    "        dist = Categorical(probs)\n",
    "        \n",
    "        if training and np.random.random() < self.epsilon:\n",
    "            # Exploration: create a uniform distribution for exploration\n",
    "            explore_probs = torch.ones(self.action_size) / self.action_size\n",
    "            explore_dist = Categorical(explore_probs)\n",
    "            a = explore_dist.sample()\n",
    "        else:\n",
    "            # Exploitation: sample from policy\n",
    "            a = dist.sample()\n",
    "        \n",
    "        # Always get log_prob from the original policy distribution for proper gradients\n",
    "        log_prob = dist.log_prob(a)\n",
    "        entropy = dist.entropy()\n",
    "        \n",
    "        return a.item(), log_prob, entropy\n",
    "    \n",
    "    def decay_epsilon(self):\n",
    "        \"\"\"Decay epsilon after each episode\"\"\"\n",
    "        self.epsilon = max(self.min_epsilon, self.epsilon * self.epsilon_decay)\n",
    "    \n",
    "    def reset_epsilon(self):\n",
    "        \"\"\"Reset epsilon to initial value\"\"\"\n",
    "        self.epsilon = self.initial_epsilon\n",
    "\n",
    "    def parameters(self):\n",
    "        return [self.W]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "734893c8-e8e9-465d-8d17-5b34f1b1d28b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-06T09:29:59.386416Z",
     "iopub.status.busy": "2025-10-06T09:29:59.386416Z",
     "iopub.status.idle": "2025-10-06T09:29:59.398947Z",
     "shell.execute_reply": "2025-10-06T09:29:59.397945Z",
     "shell.execute_reply.started": "2025-10-06T09:29:59.386416Z"
    }
   },
   "outputs": [],
   "source": [
    "def check_convergence(agent_id, policies, old_params, policy_change_history, \n",
    "                     reward_history, scores, policy_thresh, reward_thresh, window):\n",
    "    \"\"\"Check multiple convergence criteria with detailed monitoring\"\"\"\n",
    "    \n",
    "    \n",
    "    # 1. Policy parameter change\n",
    "    current_params = [p.data for p in policies[agent_id].parameters()]\n",
    "    param_change = compute_parameter_change(old_params[agent_id], current_params)\n",
    "    policy_change_history[agent_id].append(param_change)\n",
    "    \n",
    "    #print(f\"  Policy parameter change: {param_change:.6f} (threshold: {policy_thresh})\")\n",
    "    \n",
    "    # 2. Reward stability\n",
    "    reward_std = None\n",
    "    if len(scores) >= window:\n",
    "        recent_rewards = [s[agent_id] for s in scores[-window:]]\n",
    "        reward_std = np.std(recent_rewards)\n",
    "        reward_history[agent_id].append(reward_std)\n",
    "        \n",
    "        recent_avg = np.mean(recent_rewards)\n",
    "    \n",
    "    # Check convergence conditions\n",
    "    converged = False\n",
    "    policy_converged = False\n",
    "    reward_converged = False\n",
    "    \n",
    "    # Policy hasn't changed significantly\n",
    "    if len(policy_change_history[agent_id]) >= 3:\n",
    "        recent_policy_changes = policy_change_history[agent_id][-3:]\n",
    "        policy_converged = all(change < policy_thresh for change in recent_policy_changes)\n",
    "        \n",
    "        if policy_converged:\n",
    "            #print(f\"  âœ“ Policy parameters stabilized (change < {policy_thresh})\")\n",
    "            converged = True\n",
    "        else:\n",
    "            max_policy_change = max(recent_policy_changes)\n",
    "            #print(f\" Policy still changing (max change: {max_policy_change:.6f} >= {policy_thresh})\")\n",
    "\n",
    "    \n",
    "    # Rewards are stable\n",
    "    #if len(reward_history[agent_id]) >= 3 and reward_std is not None:\n",
    "    #    recent_reward_stds = reward_history[agent_id][-3:]\n",
    "    #    reward_converged = all(std < reward_thresh for std in recent_reward_stds)\n",
    "        \n",
    "    #    if reward_converged:\n",
    "    #        print(f\"  âœ“ Rewards stabilized (std < {reward_thresh})\")\n",
    "    #        converged = True\n",
    "    #    else:\n",
    "    #        max_reward_std = max(recent_reward_stds)\n",
    "    #        print(f\"  Rewards still fluctuating (max std: {max_reward_std:.6f} >= {reward_thresh})\")\n",
    "\n",
    "    \n",
    "    # Convergence summary\n",
    "    #if converged:\n",
    "        #print(f\" AGENT {agent_id} CONVERGED!\") \n",
    "        #if policy_converged and reward_converged:\n",
    "        #    print(f\"  Both policy and reward criteria satisfied\")\n",
    "        #elif policy_converged:\n",
    "        #    print(f\"  Policy criteria satisfied\")\n",
    "        #elif reward_converged:\n",
    "        #    print(f\"  Reward criteria satisfied\") \n",
    "    #print(f\"  {'='*50}\")\n",
    "    \n",
    "    return converged\n",
    "\n",
    "\n",
    "def check_round_convergence(agent_id, round_policy_changes, \n",
    "                           policy_thresh):\n",
    "    \"\"\"\n",
    "    Check BETWEEN-ROUND convergence - if an agent's policy has stabilized between rounds\n",
    "    \"\"\"\n",
    "    # Need at least 2 rounds of data to check convergence\n",
    "    if len(round_policy_changes[agent_id]) < 2:\n",
    "        return False\n",
    "    \n",
    "    # Get recent policy changes between rounds\n",
    "    recent_policy_changes = round_policy_changes[agent_id][-2:]  # Last 2 round comparisons\n",
    "    \n",
    "    # Check if policy changes are below threshold\n",
    "    policy_stable = all(change < policy_thresh for change in recent_policy_changes)\n",
    "    \n",
    "    return policy_stable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b548bd62-7c09-4c32-9263-f41768caaba9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-06T11:01:02.969538Z",
     "iopub.status.busy": "2025-10-06T11:01:02.969538Z",
     "iopub.status.idle": "2025-10-06T11:01:03.014881Z",
     "shell.execute_reply": "2025-10-06T11:01:03.013880Z",
     "shell.execute_reply.started": "2025-10-06T11:01:02.969538Z"
    }
   },
   "outputs": [],
   "source": [
    "def reinforce_multi_rwd2go_alt(env, policies, optimizers, n_episodes=100, max_t=20, gamma=0.9, batch_size=500, print_every=10):\n",
    "    \n",
    "    # Add convergence parameters\n",
    "    conv_window = 10           # episodes to check for convergence\n",
    "    window = conv_window\n",
    "    policy_change_threshold = 0.001\n",
    "    reward_change_threshold = 0.001\n",
    "    min_episodes = 10        # minimum episodes before checking convergence\n",
    "    max_rounds = 20\n",
    "    entropy_coef = 0.01\n",
    "    \n",
    "    scores_deque = deque(maxlen=window)\n",
    "    scores = []\n",
    "    \n",
    "    # For WITHIN-ROUND convergence tracking\n",
    "    old_policy_params = [None] * env.n_agents\n",
    "    policy_change_history = [[] for _ in range(env.n_agents)]\n",
    "    reward_history = [[] for _ in range(env.n_agents)]\n",
    "\n",
    "    # For BETWEEN-ROUND convergence tracking  \n",
    "    round_policy_params = [None] * env.n_agents\n",
    "    round_policy_changes = [[] for _ in range(env.n_agents)]\n",
    "    round_reward_history = [[] for _ in range(env.n_agents)]\n",
    "    \n",
    "    round_count = 0\n",
    "    all_agents_stable = False\n",
    "    \n",
    "    while round_count < max_rounds and not all_agents_stable:   \n",
    "        round_count += 1\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"=== Round {round_count} ===\")\n",
    "        print(f\"{'='*50}\")\n",
    "\n",
    "        # Store policies at the START of this round for between-round comparison\n",
    "        if round_count == 1:\n",
    "            # First round - initialize with current policies\n",
    "            round_policy_params = [\n",
    "                [p.data.clone() for p in policies[i].parameters()]\n",
    "                for i in range(env.n_agents)\n",
    "            ]\n",
    "        else:\n",
    "            # For subsequent rounds, we'll compare with previous round's policies\n",
    "            previous_round_policies = round_policy_params.copy()\n",
    "        \n",
    "        # Alternate through each agent\n",
    "        for phase in range(env.n_agents):\n",
    "            print(f\"\\n--- Training Agent {phase} ---\")\n",
    "            \n",
    "            episode_count = 0\n",
    "            agent_converged = False  # WITHIN-ROUND convergence flag\n",
    "            \n",
    "            while episode_count < n_episodes and not agent_converged:\n",
    "                episode_count += 1\n",
    "                \n",
    "                # Store old policy parameters BEFORE update\n",
    "                if old_policy_params[phase] is None:\n",
    "                    old_policy_params[phase] = [\n",
    "                        p.data.clone() for p in policies[phase].parameters()\n",
    "                    ]\n",
    "                \n",
    "                # Accumulate batch loss for current agent\n",
    "                batch_loss = 0.0\n",
    "                batch_count = 0\n",
    "                batch_value_logs = defaultdict(list)\n",
    "                batch_rewards = []\n",
    "                all_entropies = []\n",
    "                \n",
    "                # ---- collect batch of episodes ----\n",
    "                for _ in range(batch_size):\n",
    "                    states = env.reset()\n",
    "            \n",
    "                    saved_log_probs = [[] for _ in range(env.n_agents)]\n",
    "                    rewards = [[] for _ in range(env.n_agents)]\n",
    "                    saved_entropies = [[] for _ in range(env.n_agents)]\n",
    "                    epi_states = [[] for _ in range(env.n_agents)]\n",
    "                    dones = [False] * env.n_agents\n",
    "            \n",
    "                    # episode\n",
    "                    for t in range(max_t):\n",
    "                        actions, log_probs = [], []\n",
    "                        for i in range(env.n_agents):\n",
    "                            a, lp, entropy = policies[i].act(states[i])\n",
    "                            actions.append(a)\n",
    "                            log_probs.append(lp)\n",
    "                            epi_states[i].append(states[i])\n",
    "            \n",
    "                        next_states, step_rewards, dones = env.step(actions)\n",
    "                        for i in range(env.n_agents):\n",
    "                            saved_log_probs[i].append(log_probs[i])\n",
    "                            rewards[i].append(step_rewards[i])\n",
    "                            saved_entropies[i].append(entropy)  # Save per step\n",
    "            \n",
    "                        states = next_states\n",
    "                        if any(dones):\n",
    "                            for i in range(env.n_agents):\n",
    "                                epi_states[i].append(states[i])\n",
    "                            break\n",
    "        \n",
    "                    # --- Process current agent's trajectory ---\n",
    "                    if len(rewards[phase]) > 0:\n",
    "                        # rewards-to-go (Monte Carlo return)\n",
    "                        discounts = [gamma**k for k in range(len(rewards[phase]) + 1)]\n",
    "                        rewards_to_go = [\n",
    "                            sum(discounts[j] * rewards[phase][j+t] for j in range(len(rewards[phase]) - t))\n",
    "                            for t in range(len(rewards[phase]))\n",
    "                        ]\n",
    "                        rewards_to_go = torch.tensor(rewards_to_go, dtype=torch.float32)\n",
    "        \n",
    "        \n",
    "                        # --- policy loss for current agent ---\n",
    "                        pol_terms = []\n",
    "                        for lp, G in zip(saved_log_probs[phase], rewards_to_go):\n",
    "                            if isinstance(lp, torch.Tensor):\n",
    "                                pol_terms.append(-lp * G)\n",
    "        \n",
    "                        if pol_terms:\n",
    "                            ep_loss = torch.stack(pol_terms).sum()\n",
    "                            batch_loss += ep_loss\n",
    "                            batch_count += 1\n",
    "        \n",
    "                    # logging rewards per episode\n",
    "                    episode_rewards = [sum(r) for r in rewards]\n",
    "                    batch_rewards.append(episode_rewards)\n",
    "                \n",
    "                all_entropies = [ent for ep_ents in saved_entropies[phase] for ent in ep_ents]\n",
    "                mean_entropy = torch.stack(all_entropies).mean() if all_entropies else torch.tensor(0.0)\n",
    "                # Update current agent's policy\n",
    "                if batch_count > 0:\n",
    "                    #loss = (batch_loss / batch_count) - entropy_coef * mean_entropy\n",
    "                    loss = batch_loss / batch_count\n",
    "                    optimizers[phase].zero_grad()\n",
    "                    loss.backward()\n",
    "                    torch.nn.utils.clip_grad_norm_(policies[phase].parameters(), max_norm=1.0)\n",
    "                    optimizers[phase].step()\n",
    "\n",
    "                # After each episode, decay exploration\n",
    "                #policies[phase].decay_epsilon()\n",
    "                \n",
    "                # Update scores\n",
    "                avg_batch_rewards = np.mean(batch_rewards, axis=0)\n",
    "                scores_deque.append(avg_batch_rewards)\n",
    "                scores.append(avg_batch_rewards)\n",
    "                \n",
    "                # After policy update, check convergence\n",
    "                if episode_count >= min_episodes:\n",
    "                    agent_converged = check_convergence(\n",
    "                        phase, policies, old_policy_params, \n",
    "                        policy_change_history, reward_history,\n",
    "                        scores, policy_change_threshold, reward_change_threshold,\n",
    "                        conv_window\n",
    "                    )\n",
    "                    \n",
    "                    # Update old parameters for next comparison\n",
    "                    old_policy_params[phase] = [\n",
    "                        p.data.clone() for p in policies[phase].parameters()\n",
    "                    ]\n",
    "                \n",
    "                if agent_converged:\n",
    "                    print(f\"âœ“ Agent {phase} converged after {episode_count} episodes\")\n",
    "                    break\n",
    "\n",
    "                # ---- print progress ----\n",
    "                if episode_count % print_every == 0:\n",
    "                    avg_rewards = np.mean(scores_deque, axis=0) if len(scores_deque) > 0 else [0]*env.n_agents\n",
    "                    msg = f\" Agent{phase} Episode {episode_count}\"\n",
    "                    msg += f\" avgR={avg_rewards[phase]:.4f}\"\n",
    "                    print(msg)\n",
    "                    print(\" \")\n",
    "\n",
    "\n",
    "\n",
    "        # BETWEEN-ROUND convergence check (after all agents complete this round)\n",
    "        solution(env, scores, policies) \n",
    "        \n",
    "        print(f\"\\n--- Round {round_count} Policy Comparison ---\")\n",
    "        \n",
    "        if round_count > 1:  # We can only compare from round 2 onwards\n",
    "            round_stable_agents = []\n",
    "            \n",
    "            for agent_id in range(env.n_agents):\n",
    "                print(f\"\\n  Comparing Agent {agent_id} policies (Round {round_count-1} vs Round {round_count}):\")\n",
    "                \n",
    "                # Get current policy parameters\n",
    "                current_params = [p.data for p in policies[agent_id].parameters()]\n",
    "                previous_params = previous_round_policies[agent_id]\n",
    "                \n",
    "                # Compute policy change between rounds\n",
    "                policy_change = compute_parameter_change(previous_params, current_params)\n",
    "                round_policy_changes[agent_id].append(policy_change)\n",
    "                print(f\"    Policy parameter change: {policy_change:.6f} (threshold: {policy_change_threshold})\")\n",
    "                \n",
    "                # Check if this agent has stabilized between rounds\n",
    "                agent_stable = check_round_convergence(agent_id, round_policy_changes, policy_change_threshold)\n",
    "                \n",
    "                if agent_stable:\n",
    "                    round_stable_agents.append(agent_id)\n",
    "                    print(f\"  Agent {agent_id} policies stabilized between rounds\")\n",
    "                else:\n",
    "                    print(f\"  Agent {agent_id} policies still changing between rounds\")\n",
    "\n",
    "            # Check if all agents have stabilized between rounds\n",
    "            if len(round_stable_agents) == env.n_agents:\n",
    "                print(f\"\\n ALL AGENTS STABILIZED BETWEEN ROUNDS AFTER ROUND {round_count}!\") \n",
    "                all_agents_stable = True\n",
    "            else:\n",
    "                stable_count = len(round_stable_agents)\n",
    "                print(f\"\\n {stable_count}/{env.n_agents} agents stabilized between rounds\")\n",
    "\n",
    "        # Store current policies for next round comparison\n",
    "        round_policy_params = [\n",
    "            [p.data.clone() for p in policies[i].parameters()]\n",
    "            for i in range(env.n_agents)\n",
    "        ]\n",
    "        \n",
    "        print(f\"{'='*50}\")\n",
    "\n",
    "    \n",
    "    return scores\n",
    "\n",
    "\n",
    "def compute_parameter_change(old_params, new_params):\n",
    "    \"\"\"Compute average relative parameter change\"\"\"\n",
    "    total_change = 0.0\n",
    "    total_params = 0\n",
    "    \n",
    "    for old, new in zip(old_params, new_params):\n",
    "        if old.shape == new.shape:\n",
    "            change = torch.norm(new - old) / (torch.norm(old) + 1e-8)\n",
    "            total_change += change.item()\n",
    "            total_params += 1\n",
    "    \n",
    "    return total_change / total_params if total_params > 0 else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "33bb99af-99f9-4574-9498-625d2b42f990",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-03T20:42:25.236370Z",
     "iopub.status.busy": "2025-10-03T20:42:25.235344Z",
     "iopub.status.idle": "2025-10-03T20:42:25.277575Z",
     "shell.execute_reply": "2025-10-03T20:42:25.275577Z",
     "shell.execute_reply.started": "2025-10-03T20:42:25.236370Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def reinforce_multi_rwd2go_alt_2(\n",
    "    env, policies, optimizers,\n",
    "    n_episodes=1000,        \n",
    "    max_t=20,                # max steps per rollout\n",
    "    gamma=0.9,\n",
    "    batch_size=1000,         # episodes per batch update\n",
    "    print_every=1,           # print every X rollout\n",
    "):\n",
    "    \n",
    "    scores_deque = deque(maxlen=window)\n",
    "    scores = []\n",
    "\n",
    "\n",
    "    round_count = 0\n",
    "    \n",
    "    while round_count < max_rounds:\n",
    "        round_count += 1\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"=== Round {round_count} ===\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        # Alternate through each agent\n",
    "        for phase in range(env.n_agents):\n",
    "            print(f\"\\n--- Training Agent {phase} ---\")\n",
    "            \n",
    "            episode_count = 0\n",
    "            \n",
    "            while episode_count < n_episodes:\n",
    "                episode_count += 1\n",
    "                \n",
    "                # Accumulate batch loss for current agent\n",
    "                batch_loss = 0.0\n",
    "                batch_count = 0\n",
    "                batch_value_logs = defaultdict(list)\n",
    "                batch_rewards = []\n",
    "                \n",
    "                # ---- collect batch of episodes ----\n",
    "                for _ in range(batch_size):\n",
    "                    states = env.reset()\n",
    "            \n",
    "                    saved_log_probs = [[] for _ in range(env.n_agents)]\n",
    "                    rewards = [[] for _ in range(env.n_agents)]\n",
    "                    epi_states = [[] for _ in range(env.n_agents)]\n",
    "                    dones = [False] * env.n_agents\n",
    "            \n",
    "                    # episode\n",
    "                    for t in range(max_t):\n",
    "                        actions, log_probs = [], []\n",
    "                        for i in range(env.n_agents):\n",
    "                            a, lp, _ = policies[i].act(states[i])\n",
    "                            actions.append(a)\n",
    "                            log_probs.append(lp)\n",
    "                            epi_states[i].append(states[i])\n",
    "            \n",
    "                        next_states, step_rewards, dones = env.step(actions)\n",
    "                        for i in range(env.n_agents):\n",
    "                            saved_log_probs[i].append(log_probs[i])\n",
    "                            rewards[i].append(step_rewards[i])\n",
    "            \n",
    "                        states = next_states\n",
    "                        if any(dones):\n",
    "                            for i in range(env.n_agents):\n",
    "                                epi_states[i].append(states[i])\n",
    "                            break\n",
    "        \n",
    "                    # --- Process current agent's trajectory ---\n",
    "                    if len(rewards[phase]) > 0:\n",
    "                        # rewards-to-go (Monte Carlo return)\n",
    "                        discounts = [gamma**k for k in range(len(rewards[phase]) + 1)]\n",
    "                        rewards_to_go = [\n",
    "                            sum(discounts[j] * rewards[phase][j+t] for j in range(len(rewards[phase]) - t))\n",
    "                            for t in range(len(rewards[phase]))\n",
    "                        ]\n",
    "                        rewards_to_go = torch.tensor(rewards_to_go, dtype=torch.float32)\n",
    "        \n",
    "        \n",
    "                        # --- policy loss for current agent ---\n",
    "                        pol_terms = []\n",
    "                        for lp, G in zip(saved_log_probs[phase], rewards_to_go):\n",
    "                            if isinstance(lp, torch.Tensor):\n",
    "                                pol_terms.append(-lp * G)\n",
    "        \n",
    "                        if pol_terms:\n",
    "                            ep_loss = torch.stack(pol_terms).sum()\n",
    "                            batch_loss += ep_loss\n",
    "                            batch_count += 1\n",
    "        \n",
    "                    # logging rewards per episode\n",
    "                    episode_rewards = [sum(r) for r in rewards]\n",
    "                    batch_rewards.append(episode_rewards)\n",
    "                \n",
    "                # Update current agent's policy\n",
    "                if batch_count > 0:\n",
    "                    loss = batch_loss / batch_count\n",
    "                    optimizers[phase].zero_grad()\n",
    "                    loss.backward()\n",
    "                    torch.nn.utils.clip_grad_norm_(policies[phase].parameters(), max_norm=1.0)\n",
    "                    optimizers[phase].step()\n",
    "                \n",
    "                # Update scores\n",
    "                avg_batch_rewards = np.mean(batch_rewards, axis=0)\n",
    "                scores_deque.append(avg_batch_rewards)\n",
    "                scores.append(avg_batch_rewards)\n",
    "                \n",
    "                # ---- print progress ----\n",
    "                if episode_count % print_every == 0:\n",
    "                    avg_rewards = np.mean(scores_deque, axis=0) if len(scores_deque) > 0 else [0]*env.n_agents\n",
    "                    msg = f\" Agent{phase} Episode {episode_count}\"\n",
    "                    msg += f\" avgR={avg_rewards[phase]:.4f}\"\n",
    "                    print(msg)\n",
    "                    \n",
    "                    s_key = f\"Agent{phase}_start\"\n",
    "                    g_key = f\"Agent{phase}_goal\"\n",
    "                    s_val = value_logs[s_key][-1] if value_logs[s_key] else None\n",
    "                    g_val = value_logs[g_key][-1] if value_logs[g_key] else None\n",
    "                    status = \"âœ“\" if agent_converged else \"âœ—\"\n",
    "                    print(f\"   Agent{phase}{status}: V(start)={s_val:.4f} V(goal)={g_val:.4f}\")\n",
    "                    print(\" \")\n",
    "    \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807a8e18-43a4-4f9b-b56f-8704085163b3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-06T11:01:04.490178Z",
     "iopub.status.busy": "2025-10-06T11:01:04.489172Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1   \u001b[92m0\u001b[0m    \n",
      "  #     #\n",
      "    #    \n",
      "\u001b[94m1\u001b[0m     # #\n",
      "  # 0    \n",
      "----------\n",
      "\n",
      "==================================================\n",
      "=== Round 1 ===\n",
      "==================================================\n",
      "\n",
      "--- Training Agent 0 ---\n",
      " Agent0 Episode 10 avgR=-0.2530\n",
      " \n",
      " Agent0 Episode 20 avgR=0.1278\n",
      " \n",
      " Agent0 Episode 30 avgR=0.5158\n",
      " \n",
      " Agent0 Episode 40 avgR=0.5970\n",
      " \n",
      " Agent0 Episode 50 avgR=0.6342\n",
      " \n",
      " Agent0 Episode 60 avgR=0.6558\n",
      " \n",
      " Agent0 Episode 70 avgR=0.6620\n",
      " \n",
      " Agent0 Episode 80 avgR=0.6758\n",
      " \n",
      " Agent0 Episode 90 avgR=0.6932\n",
      " \n",
      " Agent0 Episode 100 avgR=0.6968\n",
      " \n",
      "\n",
      "--- Training Agent 1 ---\n",
      " Agent1 Episode 10 avgR=-0.1244\n",
      " \n",
      " Agent1 Episode 20 avgR=0.8866\n",
      " \n",
      " Agent1 Episode 30 avgR=0.9890\n",
      " \n",
      " Agent1 Episode 40 avgR=0.9990\n",
      " \n",
      " Agent1 Episode 50 avgR=0.9988\n",
      " \n",
      " Agent1 Episode 60 avgR=0.9998\n",
      " \n",
      "  âœ“ Policy parameters stabilized (change < 0.001)\n",
      "  ðŸŽ¯ AGENT 1 CONVERGED! ðŸŽ¯\n",
      "  Policy criteria satisfied\n",
      "âœ“ Agent 1 converged after 68 episodes\n",
      "1   \u001b[92m0\u001b[0m    \n",
      "  #     #\n",
      "    #    \n",
      "\u001b[94m1\u001b[0m     # #\n",
      "  # 0    \n",
      "----------\n",
      "    \u001b[92m0\u001b[0m    \n",
      "1 #     #\n",
      "    #    \n",
      "\u001b[94m1\u001b[0m   0 # #\n",
      "  #      \n",
      "----------\n",
      "    \u001b[92m0\u001b[0m    \n",
      "  #     #\n",
      "1   #    \n",
      "\u001b[94m1\u001b[0m 0   # #\n",
      "  #      \n",
      "----------\n",
      "    \u001b[92m0\u001b[0m    \n",
      "  #     #\n",
      "  0 #    \n",
      "1     # #\n",
      "  #      \n",
      "----------\n",
      "\n",
      "--- Round 1 Policy Comparison ---\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "=== Round 2 ===\n",
      "==================================================\n",
      "\n",
      "--- Training Agent 0 ---\n",
      " Agent0 Episode 10 avgR=-0.9994\n",
      " \n",
      " Agent0 Episode 20 avgR=-0.9996\n",
      " \n",
      "  âœ“ Policy parameters stabilized (change < 0.001)\n",
      "  ðŸŽ¯ AGENT 0 CONVERGED! ðŸŽ¯\n",
      "  Policy criteria satisfied\n",
      "âœ“ Agent 0 converged after 27 episodes\n",
      "\n",
      "--- Training Agent 1 ---\n",
      " Agent1 Episode 10 avgR=0.9998\n",
      " \n",
      " Agent1 Episode 20 avgR=1.0000\n",
      " \n",
      "  âœ“ Policy parameters stabilized (change < 0.001)\n",
      "  ðŸŽ¯ AGENT 1 CONVERGED! ðŸŽ¯\n",
      "  Policy criteria satisfied\n",
      "âœ“ Agent 1 converged after 26 episodes\n",
      "1   \u001b[92m0\u001b[0m    \n",
      "  #     #\n",
      "    #    \n",
      "\u001b[94m1\u001b[0m     # #\n",
      "  # 0    \n",
      "----------\n",
      "    \u001b[92m0\u001b[0m    \n",
      "1 #     #\n",
      "    #    \n",
      "\u001b[94m1\u001b[0m     # #\n",
      "  # 0    \n",
      "----------\n",
      "    \u001b[92m0\u001b[0m    \n",
      "  #     #\n",
      "1   #    \n",
      "\u001b[94m1\u001b[0m     # #\n",
      "  # 0    \n",
      "----------\n",
      "    \u001b[92m0\u001b[0m    \n",
      "  #     #\n",
      "    #    \n",
      "1     # #\n",
      "  # 0    \n",
      "----------\n",
      "\n",
      "--- Round 2 Policy Comparison ---\n",
      "\n",
      "  Comparing Agent 0 policies (Round 1 vs Round 2):\n",
      "    Policy parameter change: 0.110605 (threshold: 0.001)\n",
      "    ðŸ”„ Agent 0 policies still changing between rounds\n",
      "\n",
      "  Comparing Agent 1 policies (Round 1 vs Round 2):\n",
      "    Policy parameter change: 0.025524 (threshold: 0.001)\n",
      "    ðŸ”„ Agent 1 policies still changing between rounds\n",
      "\n",
      "ðŸ”„ 0/2 agents stabilized between rounds\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "=== Round 3 ===\n",
      "==================================================\n",
      "\n",
      "--- Training Agent 0 ---\n",
      " Agent0 Episode 10 avgR=-0.9998\n",
      " \n",
      "  âœ“ Policy parameters stabilized (change < 0.001)\n",
      "  ðŸŽ¯ AGENT 0 CONVERGED! ðŸŽ¯\n",
      "  Policy criteria satisfied\n",
      "âœ“ Agent 0 converged after 13 episodes\n",
      "\n",
      "--- Training Agent 1 ---\n",
      " Agent1 Episode 10 avgR=0.9998\n",
      " \n",
      "  âœ“ Policy parameters stabilized (change < 0.001)\n",
      "  ðŸŽ¯ AGENT 1 CONVERGED! ðŸŽ¯\n",
      "  Policy criteria satisfied\n",
      "âœ“ Agent 1 converged after 13 episodes\n",
      "1   \u001b[92m0\u001b[0m    \n",
      "  #     #\n",
      "    #    \n",
      "\u001b[94m1\u001b[0m     # #\n",
      "  # 0    \n",
      "----------\n",
      "    \u001b[92m0\u001b[0m    \n",
      "1 #     #\n",
      "    #    \n",
      "\u001b[94m1\u001b[0m     # #\n",
      "  # 0    \n",
      "----------\n",
      "    \u001b[92m0\u001b[0m    \n",
      "  #     #\n",
      "1   #    \n",
      "\u001b[94m1\u001b[0m     # #\n",
      "  # 0    \n",
      "----------\n",
      "    \u001b[92m0\u001b[0m    \n",
      "  #     #\n",
      "    #    \n",
      "1     # #\n",
      "  # 0    \n",
      "----------\n",
      "\n",
      "--- Round 3 Policy Comparison ---\n",
      "\n",
      "  Comparing Agent 0 policies (Round 2 vs Round 3):\n",
      "    Policy parameter change: 0.006639 (threshold: 0.001)\n",
      "    ðŸ”„ Agent 0 policies still changing between rounds\n",
      "\n",
      "  Comparing Agent 1 policies (Round 2 vs Round 3):\n",
      "    Policy parameter change: 0.007473 (threshold: 0.001)\n",
      "    ðŸ”„ Agent 1 policies still changing between rounds\n",
      "\n",
      "ðŸ”„ 0/2 agents stabilized between rounds\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "=== Round 4 ===\n",
      "==================================================\n",
      "\n",
      "--- Training Agent 0 ---\n",
      " Agent0 Episode 10 avgR=-0.9994\n",
      " \n",
      "  âœ“ Policy parameters stabilized (change < 0.001)\n",
      "  ðŸŽ¯ AGENT 0 CONVERGED! ðŸŽ¯\n",
      "  Policy criteria satisfied\n",
      "âœ“ Agent 0 converged after 13 episodes\n",
      "\n",
      "--- Training Agent 1 ---\n",
      " Agent1 Episode 10 avgR=0.9998\n",
      " \n",
      "  âœ“ Policy parameters stabilized (change < 0.001)\n",
      "  ðŸŽ¯ AGENT 1 CONVERGED! ðŸŽ¯\n",
      "  Policy criteria satisfied\n",
      "âœ“ Agent 1 converged after 13 episodes\n",
      "1   \u001b[92m0\u001b[0m    \n",
      "  #     #\n",
      "    #    \n",
      "\u001b[94m1\u001b[0m     # #\n",
      "  # 0    \n",
      "----------\n",
      "    \u001b[92m0\u001b[0m    \n",
      "1 #     #\n",
      "    #    \n",
      "\u001b[94m1\u001b[0m     # #\n",
      "  # 0    \n",
      "----------\n",
      "    \u001b[92m0\u001b[0m    \n",
      "  #     #\n",
      "1   #    \n",
      "\u001b[94m1\u001b[0m     # #\n",
      "  # 0    \n",
      "----------\n",
      "    \u001b[92m0\u001b[0m    \n",
      "  #     #\n",
      "    #    \n",
      "1     # #\n",
      "  # 0    \n",
      "----------\n",
      "\n",
      "--- Round 4 Policy Comparison ---\n",
      "\n",
      "  Comparing Agent 0 policies (Round 3 vs Round 4):\n",
      "    Policy parameter change: 0.001046 (threshold: 0.001)\n",
      "    ðŸ”„ Agent 0 policies still changing between rounds\n",
      "\n",
      "  Comparing Agent 1 policies (Round 3 vs Round 4):\n",
      "    Policy parameter change: 0.006737 (threshold: 0.001)\n",
      "    ðŸ”„ Agent 1 policies still changing between rounds\n",
      "\n",
      "ðŸ”„ 0/2 agents stabilized between rounds\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "=== Round 5 ===\n",
      "==================================================\n",
      "\n",
      "--- Training Agent 0 ---\n",
      " Agent0 Episode 10 avgR=-1.0000\n",
      " \n",
      "  âœ“ Policy parameters stabilized (change < 0.001)\n",
      "  ðŸŽ¯ AGENT 0 CONVERGED! ðŸŽ¯\n",
      "  Policy criteria satisfied\n",
      "âœ“ Agent 0 converged after 13 episodes\n",
      "\n",
      "--- Training Agent 1 ---\n",
      " Agent1 Episode 10 avgR=0.9996\n",
      " \n",
      "  âœ“ Policy parameters stabilized (change < 0.001)\n",
      "  ðŸŽ¯ AGENT 1 CONVERGED! ðŸŽ¯\n",
      "  Policy criteria satisfied\n",
      "âœ“ Agent 1 converged after 13 episodes\n",
      "1   \u001b[92m0\u001b[0m    \n",
      "  #     #\n",
      "    #    \n",
      "\u001b[94m1\u001b[0m     # #\n",
      "  # 0    \n",
      "----------\n",
      "    \u001b[92m0\u001b[0m    \n",
      "1 #     #\n",
      "    #    \n",
      "\u001b[94m1\u001b[0m     # #\n",
      "  # 0    \n",
      "----------\n",
      "    \u001b[92m0\u001b[0m    \n",
      "  #     #\n",
      "1   #    \n",
      "\u001b[94m1\u001b[0m     # #\n",
      "  # 0    \n",
      "----------\n",
      "    \u001b[92m0\u001b[0m    \n",
      "  #     #\n",
      "    #    \n",
      "1     # #\n",
      "  # 0    \n",
      "----------\n",
      "\n",
      "--- Round 5 Policy Comparison ---\n",
      "\n",
      "  Comparing Agent 0 policies (Round 4 vs Round 5):\n",
      "    Policy parameter change: 0.004955 (threshold: 0.001)\n",
      "    ðŸ”„ Agent 0 policies still changing between rounds\n",
      "\n",
      "  Comparing Agent 1 policies (Round 4 vs Round 5):\n",
      "    Policy parameter change: 0.009827 (threshold: 0.001)\n",
      "    ðŸ”„ Agent 1 policies still changing between rounds\n",
      "\n",
      "ðŸ”„ 0/2 agents stabilized between rounds\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "=== Round 6 ===\n",
      "==================================================\n",
      "\n",
      "--- Training Agent 0 ---\n",
      " Agent0 Episode 10 avgR=-0.9998\n",
      " \n",
      "  âœ“ Policy parameters stabilized (change < 0.001)\n",
      "  ðŸŽ¯ AGENT 0 CONVERGED! ðŸŽ¯\n",
      "  Policy criteria satisfied\n",
      "âœ“ Agent 0 converged after 13 episodes\n",
      "\n",
      "--- Training Agent 1 ---\n",
      " Agent1 Episode 10 avgR=0.9996\n",
      " \n",
      "  âœ“ Policy parameters stabilized (change < 0.001)\n",
      "  ðŸŽ¯ AGENT 1 CONVERGED! ðŸŽ¯\n",
      "  Policy criteria satisfied\n",
      "âœ“ Agent 1 converged after 13 episodes\n",
      "1   \u001b[92m0\u001b[0m    \n",
      "  #     #\n",
      "    #    \n",
      "\u001b[94m1\u001b[0m     # #\n",
      "  # 0    \n",
      "----------\n",
      "    \u001b[92m0\u001b[0m    \n",
      "1 #     #\n",
      "    #    \n",
      "\u001b[94m1\u001b[0m     # #\n",
      "  # 0    \n",
      "----------\n",
      "    \u001b[92m0\u001b[0m    \n",
      "  #     #\n",
      "1   #    \n",
      "\u001b[94m1\u001b[0m     # #\n",
      "  # 0    \n",
      "----------\n",
      "    \u001b[92m0\u001b[0m    \n",
      "  #     #\n",
      "    #    \n",
      "1     # #\n",
      "  # 0    \n",
      "----------\n",
      "\n",
      "--- Round 6 Policy Comparison ---\n",
      "\n",
      "  Comparing Agent 0 policies (Round 5 vs Round 6):\n",
      "    Policy parameter change: 0.004057 (threshold: 0.001)\n",
      "    ðŸ”„ Agent 0 policies still changing between rounds\n",
      "\n",
      "  Comparing Agent 1 policies (Round 5 vs Round 6):\n",
      "    Policy parameter change: 0.011882 (threshold: 0.001)\n",
      "    ðŸ”„ Agent 1 policies still changing between rounds\n",
      "\n",
      "ðŸ”„ 0/2 agents stabilized between rounds\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "=== Round 7 ===\n",
      "==================================================\n",
      "\n",
      "--- Training Agent 0 ---\n",
      " Agent0 Episode 10 avgR=-0.9998\n",
      " \n",
      "  âœ“ Policy parameters stabilized (change < 0.001)\n",
      "  ðŸŽ¯ AGENT 0 CONVERGED! ðŸŽ¯\n",
      "  Policy criteria satisfied\n",
      "âœ“ Agent 0 converged after 13 episodes\n",
      "\n",
      "--- Training Agent 1 ---\n",
      " Agent1 Episode 10 avgR=0.9996\n",
      " \n",
      "  âœ“ Policy parameters stabilized (change < 0.001)\n",
      "  ðŸŽ¯ AGENT 1 CONVERGED! ðŸŽ¯\n",
      "  Policy criteria satisfied\n",
      "âœ“ Agent 1 converged after 13 episodes\n",
      "1   \u001b[92m0\u001b[0m    \n",
      "  #     #\n",
      "    #    \n",
      "\u001b[94m1\u001b[0m     # #\n",
      "  # 0    \n",
      "----------\n",
      "    \u001b[92m0\u001b[0m    \n",
      "1 #     #\n",
      "    #    \n",
      "\u001b[94m1\u001b[0m     # #\n",
      "  # 0    \n",
      "----------\n",
      "    \u001b[92m0\u001b[0m    \n",
      "  #     #\n",
      "1   #    \n",
      "\u001b[94m1\u001b[0m     # #\n",
      "  # 0    \n",
      "----------\n",
      "    \u001b[92m0\u001b[0m    \n",
      "  #     #\n",
      "    #    \n",
      "1     # #\n",
      "  # 0    \n",
      "----------\n",
      "\n",
      "--- Round 7 Policy Comparison ---\n",
      "\n",
      "  Comparing Agent 0 policies (Round 6 vs Round 7):\n",
      "    Policy parameter change: 0.002145 (threshold: 0.001)\n",
      "    ðŸ”„ Agent 0 policies still changing between rounds\n",
      "\n",
      "  Comparing Agent 1 policies (Round 6 vs Round 7):\n",
      "    Policy parameter change: 0.008512 (threshold: 0.001)\n",
      "    ðŸ”„ Agent 1 policies still changing between rounds\n",
      "\n",
      "ðŸ”„ 0/2 agents stabilized between rounds\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "=== Round 8 ===\n",
      "==================================================\n",
      "\n",
      "--- Training Agent 0 ---\n",
      " Agent0 Episode 10 avgR=-0.9996\n",
      " \n",
      "  âœ“ Policy parameters stabilized (change < 0.001)\n",
      "  ðŸŽ¯ AGENT 0 CONVERGED! ðŸŽ¯\n",
      "  Policy criteria satisfied\n",
      "âœ“ Agent 0 converged after 13 episodes\n",
      "\n",
      "--- Training Agent 1 ---\n",
      " Agent1 Episode 10 avgR=1.0000\n",
      " \n",
      "  âœ“ Policy parameters stabilized (change < 0.001)\n",
      "  ðŸŽ¯ AGENT 1 CONVERGED! ðŸŽ¯\n",
      "  Policy criteria satisfied\n",
      "âœ“ Agent 1 converged after 13 episodes\n",
      "1   \u001b[92m0\u001b[0m    \n",
      "  #     #\n",
      "    #    \n",
      "\u001b[94m1\u001b[0m     # #\n",
      "  # 0    \n",
      "----------\n",
      "    \u001b[92m0\u001b[0m    \n",
      "1 #     #\n",
      "    #    \n",
      "\u001b[94m1\u001b[0m     # #\n",
      "  # 0    \n",
      "----------\n",
      "    \u001b[92m0\u001b[0m    \n",
      "  #     #\n",
      "1   #    \n",
      "\u001b[94m1\u001b[0m     # #\n",
      "  # 0    \n",
      "----------\n",
      "    \u001b[92m0\u001b[0m    \n",
      "  #     #\n",
      "    #    \n",
      "1     # #\n",
      "  # 0    \n",
      "----------\n",
      "\n",
      "--- Round 8 Policy Comparison ---\n",
      "\n",
      "  Comparing Agent 0 policies (Round 7 vs Round 8):\n",
      "    Policy parameter change: 0.001628 (threshold: 0.001)\n",
      "    ðŸ”„ Agent 0 policies still changing between rounds\n",
      "\n",
      "  Comparing Agent 1 policies (Round 7 vs Round 8):\n",
      "    Policy parameter change: 0.003466 (threshold: 0.001)\n",
      "    ðŸ”„ Agent 1 policies still changing between rounds\n",
      "\n",
      "ðŸ”„ 0/2 agents stabilized between rounds\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "=== Round 9 ===\n",
      "==================================================\n",
      "\n",
      "--- Training Agent 0 ---\n",
      " Agent0 Episode 10 avgR=-0.9996\n",
      " \n",
      "  âœ“ Policy parameters stabilized (change < 0.001)\n",
      "  ðŸŽ¯ AGENT 0 CONVERGED! ðŸŽ¯\n",
      "  Policy criteria satisfied\n",
      "âœ“ Agent 0 converged after 13 episodes\n",
      "\n",
      "--- Training Agent 1 ---\n",
      " Agent1 Episode 10 avgR=0.9998\n",
      " \n",
      "  âœ“ Policy parameters stabilized (change < 0.001)\n",
      "  ðŸŽ¯ AGENT 1 CONVERGED! ðŸŽ¯\n",
      "  Policy criteria satisfied\n",
      "âœ“ Agent 1 converged after 13 episodes\n",
      "1   \u001b[92m0\u001b[0m    \n",
      "  #     #\n",
      "    #    \n",
      "\u001b[94m1\u001b[0m     # #\n",
      "  # 0    \n",
      "----------\n",
      "    \u001b[92m0\u001b[0m    \n",
      "1 #     #\n",
      "    #    \n",
      "\u001b[94m1\u001b[0m     # #\n",
      "  # 0    \n",
      "----------\n",
      "    \u001b[92m0\u001b[0m    \n",
      "  #     #\n",
      "1   #    \n",
      "\u001b[94m1\u001b[0m     # #\n",
      "  # 0    \n",
      "----------\n",
      "    \u001b[92m0\u001b[0m    \n",
      "  #     #\n",
      "    #    \n",
      "1     # #\n",
      "  # 0    \n",
      "----------\n",
      "\n",
      "--- Round 9 Policy Comparison ---\n",
      "\n",
      "  Comparing Agent 0 policies (Round 8 vs Round 9):\n",
      "    Policy parameter change: 0.003431 (threshold: 0.001)\n",
      "    ðŸ”„ Agent 0 policies still changing between rounds\n",
      "\n",
      "  Comparing Agent 1 policies (Round 8 vs Round 9):\n",
      "    Policy parameter change: 0.004974 (threshold: 0.001)\n",
      "    ðŸ”„ Agent 1 policies still changing between rounds\n",
      "\n",
      "ðŸ”„ 0/2 agents stabilized between rounds\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "=== Round 10 ===\n",
      "==================================================\n",
      "\n",
      "--- Training Agent 0 ---\n",
      " Agent0 Episode 10 avgR=-1.0000\n",
      " \n",
      "  âœ“ Policy parameters stabilized (change < 0.001)\n",
      "  ðŸŽ¯ AGENT 0 CONVERGED! ðŸŽ¯\n",
      "  Policy criteria satisfied\n",
      "âœ“ Agent 0 converged after 13 episodes\n",
      "\n",
      "--- Training Agent 1 ---\n",
      " Agent1 Episode 10 avgR=1.0000\n",
      " \n",
      "  âœ“ Policy parameters stabilized (change < 0.001)\n",
      "  ðŸŽ¯ AGENT 1 CONVERGED! ðŸŽ¯\n",
      "  Policy criteria satisfied\n",
      "âœ“ Agent 1 converged after 13 episodes\n",
      "1   \u001b[92m0\u001b[0m    \n",
      "  #     #\n",
      "    #    \n",
      "\u001b[94m1\u001b[0m     # #\n",
      "  # 0    \n",
      "----------\n",
      "    \u001b[92m0\u001b[0m    \n",
      "1 #     #\n",
      "    #    \n",
      "\u001b[94m1\u001b[0m     # #\n",
      "  # 0    \n",
      "----------\n",
      "    \u001b[92m0\u001b[0m    \n",
      "  #     #\n",
      "1   #    \n",
      "\u001b[94m1\u001b[0m     # #\n",
      "  # 0    \n",
      "----------\n",
      "    \u001b[92m0\u001b[0m    \n",
      "  #     #\n",
      "    #    \n",
      "1     # #\n",
      "  # 0    \n",
      "----------\n",
      "\n",
      "--- Round 10 Policy Comparison ---\n",
      "\n",
      "  Comparing Agent 0 policies (Round 9 vs Round 10):\n",
      "    Policy parameter change: 0.002339 (threshold: 0.001)\n",
      "    ðŸ”„ Agent 0 policies still changing between rounds\n",
      "\n",
      "  Comparing Agent 1 policies (Round 9 vs Round 10):\n",
      "    Policy parameter change: 0.005725 (threshold: 0.001)\n",
      "    ðŸ”„ Agent 1 policies still changing between rounds\n",
      "\n",
      "ðŸ”„ 0/2 agents stabilized between rounds\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "=== Round 11 ===\n",
      "==================================================\n",
      "\n",
      "--- Training Agent 0 ---\n",
      " Agent0 Episode 10 avgR=-0.9996\n",
      " \n",
      "  âœ“ Policy parameters stabilized (change < 0.001)\n",
      "  ðŸŽ¯ AGENT 0 CONVERGED! ðŸŽ¯\n",
      "  Policy criteria satisfied\n",
      "âœ“ Agent 0 converged after 17 episodes\n",
      "\n",
      "--- Training Agent 1 ---\n",
      " Agent1 Episode 10 avgR=0.9996\n",
      " \n",
      "  âœ“ Policy parameters stabilized (change < 0.001)\n",
      "  ðŸŽ¯ AGENT 1 CONVERGED! ðŸŽ¯\n",
      "  Policy criteria satisfied\n",
      "âœ“ Agent 1 converged after 13 episodes\n",
      "1   \u001b[92m0\u001b[0m    \n",
      "  #     #\n",
      "    #    \n",
      "\u001b[94m1\u001b[0m     # #\n",
      "  # 0    \n",
      "----------\n",
      "    \u001b[92m0\u001b[0m    \n",
      "1 #     #\n",
      "    #    \n",
      "\u001b[94m1\u001b[0m     # #\n",
      "  # 0    \n",
      "----------\n",
      "    \u001b[92m0\u001b[0m    \n",
      "  #     #\n",
      "1   #    \n",
      "\u001b[94m1\u001b[0m     # #\n",
      "  # 0    \n",
      "----------\n",
      "    \u001b[92m0\u001b[0m    \n",
      "  #     #\n",
      "    #    \n",
      "1     # #\n",
      "  # 0    \n",
      "----------\n",
      "\n",
      "--- Round 11 Policy Comparison ---\n",
      "\n",
      "  Comparing Agent 0 policies (Round 10 vs Round 11):\n",
      "    Policy parameter change: 0.011058 (threshold: 0.001)\n",
      "    ðŸ”„ Agent 0 policies still changing between rounds\n",
      "\n",
      "  Comparing Agent 1 policies (Round 10 vs Round 11):\n",
      "    Policy parameter change: 0.002325 (threshold: 0.001)\n",
      "    ðŸ”„ Agent 1 policies still changing between rounds\n",
      "\n",
      "ðŸ”„ 0/2 agents stabilized between rounds\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "=== Round 12 ===\n",
      "==================================================\n",
      "\n",
      "--- Training Agent 0 ---\n",
      " Agent0 Episode 10 avgR=-1.0000\n",
      " \n",
      "  âœ“ Policy parameters stabilized (change < 0.001)\n",
      "  ðŸŽ¯ AGENT 0 CONVERGED! ðŸŽ¯\n",
      "  Policy criteria satisfied\n",
      "âœ“ Agent 0 converged after 15 episodes\n",
      "\n",
      "--- Training Agent 1 ---\n",
      " Agent1 Episode 10 avgR=1.0000\n",
      " \n",
      "  âœ“ Policy parameters stabilized (change < 0.001)\n",
      "  ðŸŽ¯ AGENT 1 CONVERGED! ðŸŽ¯\n",
      "  Policy criteria satisfied\n",
      "âœ“ Agent 1 converged after 13 episodes\n",
      "1   \u001b[92m0\u001b[0m    \n",
      "  #     #\n",
      "    #    \n",
      "\u001b[94m1\u001b[0m     # #\n",
      "  # 0    \n",
      "----------\n",
      "    \u001b[92m0\u001b[0m    \n",
      "1 #     #\n",
      "    #    \n",
      "\u001b[94m1\u001b[0m     # #\n",
      "  # 0    \n",
      "----------\n",
      "    \u001b[92m0\u001b[0m    \n",
      "  #     #\n",
      "1   #    \n",
      "\u001b[94m1\u001b[0m     # #\n",
      "  # 0    \n",
      "----------\n",
      "    \u001b[92m0\u001b[0m    \n",
      "  #     #\n",
      "    #    \n",
      "1     # #\n",
      "  # 0    \n",
      "----------\n",
      "\n",
      "--- Round 12 Policy Comparison ---\n",
      "\n",
      "  Comparing Agent 0 policies (Round 11 vs Round 12):\n",
      "    Policy parameter change: 0.003665 (threshold: 0.001)\n",
      "    ðŸ”„ Agent 0 policies still changing between rounds\n",
      "\n",
      "  Comparing Agent 1 policies (Round 11 vs Round 12):\n",
      "    Policy parameter change: 0.003466 (threshold: 0.001)\n",
      "    ðŸ”„ Agent 1 policies still changing between rounds\n",
      "\n",
      "ðŸ”„ 0/2 agents stabilized between rounds\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "=== Round 13 ===\n",
      "==================================================\n",
      "\n",
      "--- Training Agent 0 ---\n",
      " Agent0 Episode 10 avgR=-0.9998\n",
      " \n",
      "  âœ“ Policy parameters stabilized (change < 0.001)\n",
      "  ðŸŽ¯ AGENT 0 CONVERGED! ðŸŽ¯\n",
      "  Policy criteria satisfied\n",
      "âœ“ Agent 0 converged after 13 episodes\n",
      "\n",
      "--- Training Agent 1 ---\n",
      " Agent1 Episode 10 avgR=0.9998\n",
      " \n",
      "  âœ“ Policy parameters stabilized (change < 0.001)\n",
      "  ðŸŽ¯ AGENT 1 CONVERGED! ðŸŽ¯\n",
      "  Policy criteria satisfied\n",
      "âœ“ Agent 1 converged after 13 episodes\n",
      "1   \u001b[92m0\u001b[0m    \n",
      "  #     #\n",
      "    #    \n",
      "\u001b[94m1\u001b[0m     # #\n",
      "  # 0    \n",
      "----------\n",
      "    \u001b[92m0\u001b[0m    \n",
      "1 #     #\n",
      "    #    \n",
      "\u001b[94m1\u001b[0m     # #\n",
      "  # 0    \n",
      "----------\n",
      "    \u001b[92m0\u001b[0m    \n",
      "  #     #\n",
      "1   #    \n",
      "\u001b[94m1\u001b[0m     # #\n",
      "  # 0    \n",
      "----------\n",
      "    \u001b[92m0\u001b[0m    \n",
      "  #     #\n",
      "    #    \n",
      "1     # #\n",
      "  # 0    \n",
      "----------\n",
      "\n",
      "--- Round 13 Policy Comparison ---\n",
      "\n",
      "  Comparing Agent 0 policies (Round 12 vs Round 13):\n",
      "    Policy parameter change: 0.007352 (threshold: 0.001)\n",
      "    ðŸ”„ Agent 0 policies still changing between rounds\n",
      "\n",
      "  Comparing Agent 1 policies (Round 12 vs Round 13):\n",
      "    Policy parameter change: 0.002105 (threshold: 0.001)\n",
      "    ðŸ”„ Agent 1 policies still changing between rounds\n",
      "\n",
      "ðŸ”„ 0/2 agents stabilized between rounds\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "=== Round 14 ===\n",
      "==================================================\n",
      "\n",
      "--- Training Agent 0 ---\n",
      " Agent0 Episode 10 avgR=-0.9998\n",
      " \n",
      "  âœ“ Policy parameters stabilized (change < 0.001)\n",
      "  ðŸŽ¯ AGENT 0 CONVERGED! ðŸŽ¯\n",
      "  Policy criteria satisfied\n",
      "âœ“ Agent 0 converged after 13 episodes\n",
      "\n",
      "--- Training Agent 1 ---\n",
      " Agent1 Episode 10 avgR=1.0000\n",
      " \n",
      "  âœ“ Policy parameters stabilized (change < 0.001)\n",
      "  ðŸŽ¯ AGENT 1 CONVERGED! ðŸŽ¯\n",
      "  Policy criteria satisfied\n",
      "âœ“ Agent 1 converged after 13 episodes\n",
      "1   \u001b[92m0\u001b[0m    \n",
      "  #     #\n",
      "    #    \n",
      "\u001b[94m1\u001b[0m     # #\n",
      "  # 0    \n",
      "----------\n",
      "    \u001b[92m0\u001b[0m    \n",
      "1 #     #\n",
      "    #    \n",
      "\u001b[94m1\u001b[0m     # #\n",
      "  # 0    \n",
      "----------\n",
      "    \u001b[92m0\u001b[0m    \n",
      "  #     #\n",
      "1   #    \n",
      "\u001b[94m1\u001b[0m     # #\n",
      "  # 0    \n",
      "----------\n",
      "    \u001b[92m0\u001b[0m    \n",
      "  #     #\n",
      "    #    \n",
      "1     # #\n",
      "  # 0    \n",
      "----------\n",
      "\n",
      "--- Round 14 Policy Comparison ---\n",
      "\n",
      "  Comparing Agent 0 policies (Round 13 vs Round 14):\n",
      "    Policy parameter change: 0.002170 (threshold: 0.001)\n",
      "    ðŸ”„ Agent 0 policies still changing between rounds\n",
      "\n",
      "  Comparing Agent 1 policies (Round 13 vs Round 14):\n",
      "    Policy parameter change: 0.005653 (threshold: 0.001)\n",
      "    ðŸ”„ Agent 1 policies still changing between rounds\n",
      "\n",
      "ðŸ”„ 0/2 agents stabilized between rounds\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "=== Round 15 ===\n",
      "==================================================\n",
      "\n",
      "--- Training Agent 0 ---\n",
      " Agent0 Episode 10 avgR=-1.0000\n",
      " \n",
      "  âœ“ Policy parameters stabilized (change < 0.001)\n",
      "  ðŸŽ¯ AGENT 0 CONVERGED! ðŸŽ¯\n",
      "  Policy criteria satisfied\n",
      "âœ“ Agent 0 converged after 13 episodes\n",
      "\n",
      "--- Training Agent 1 ---\n",
      " Agent1 Episode 10 avgR=1.0000\n",
      " \n",
      "  âœ“ Policy parameters stabilized (change < 0.001)\n",
      "  ðŸŽ¯ AGENT 1 CONVERGED! ðŸŽ¯\n",
      "  Policy criteria satisfied\n",
      "âœ“ Agent 1 converged after 13 episodes\n",
      "1   \u001b[92m0\u001b[0m    \n",
      "  #     #\n",
      "    #    \n",
      "\u001b[94m1\u001b[0m     # #\n",
      "  # 0    \n",
      "----------\n",
      "    \u001b[92m0\u001b[0m    \n",
      "1 #     #\n",
      "    #    \n",
      "\u001b[94m1\u001b[0m     # #\n",
      "  # 0    \n",
      "----------\n",
      "    \u001b[92m0\u001b[0m    \n",
      "  #     #\n",
      "1   #    \n",
      "\u001b[94m1\u001b[0m     # #\n",
      "  # 0    \n",
      "----------\n",
      "    \u001b[92m0\u001b[0m    \n",
      "  #     #\n",
      "    #    \n",
      "1     # #\n",
      "  # 0    \n",
      "----------\n",
      "\n",
      "--- Round 15 Policy Comparison ---\n",
      "\n",
      "  Comparing Agent 0 policies (Round 14 vs Round 15):\n",
      "    Policy parameter change: 0.006043 (threshold: 0.001)\n",
      "    ðŸ”„ Agent 0 policies still changing between rounds\n",
      "\n",
      "  Comparing Agent 1 policies (Round 14 vs Round 15):\n",
      "    Policy parameter change: 0.007128 (threshold: 0.001)\n",
      "    ðŸ”„ Agent 1 policies still changing between rounds\n",
      "\n",
      "ðŸ”„ 0/2 agents stabilized between rounds\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "=== Round 16 ===\n",
      "==================================================\n",
      "\n",
      "--- Training Agent 0 ---\n",
      " Agent0 Episode 10 avgR=-0.9998\n",
      " \n",
      "  âœ“ Policy parameters stabilized (change < 0.001)\n",
      "  ðŸŽ¯ AGENT 0 CONVERGED! ðŸŽ¯\n",
      "  Policy criteria satisfied\n",
      "âœ“ Agent 0 converged after 13 episodes\n",
      "\n",
      "--- Training Agent 1 ---\n",
      " Agent1 Episode 10 avgR=1.0000\n",
      " \n",
      "  âœ“ Policy parameters stabilized (change < 0.001)\n",
      "  ðŸŽ¯ AGENT 1 CONVERGED! ðŸŽ¯\n",
      "  Policy criteria satisfied\n",
      "âœ“ Agent 1 converged after 13 episodes\n",
      "1   \u001b[92m0\u001b[0m    \n",
      "  #     #\n",
      "    #    \n",
      "\u001b[94m1\u001b[0m     # #\n",
      "  # 0    \n",
      "----------\n",
      "    \u001b[92m0\u001b[0m    \n",
      "1 #     #\n",
      "    #    \n",
      "\u001b[94m1\u001b[0m     # #\n",
      "  # 0    \n",
      "----------\n",
      "    \u001b[92m0\u001b[0m    \n",
      "  #     #\n",
      "1   #    \n",
      "\u001b[94m1\u001b[0m     # #\n",
      "  # 0    \n",
      "----------\n",
      "    \u001b[92m0\u001b[0m    \n",
      "  #     #\n",
      "    #    \n",
      "1     # #\n",
      "  # 0    \n",
      "----------\n",
      "\n",
      "--- Round 16 Policy Comparison ---\n",
      "\n",
      "  Comparing Agent 0 policies (Round 15 vs Round 16):\n",
      "    Policy parameter change: 0.003020 (threshold: 0.001)\n",
      "    ðŸ”„ Agent 0 policies still changing between rounds\n",
      "\n",
      "  Comparing Agent 1 policies (Round 15 vs Round 16):\n",
      "    Policy parameter change: 0.005381 (threshold: 0.001)\n",
      "    ðŸ”„ Agent 1 policies still changing between rounds\n",
      "\n",
      "ðŸ”„ 0/2 agents stabilized between rounds\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "=== Round 17 ===\n",
      "==================================================\n",
      "\n",
      "--- Training Agent 0 ---\n",
      " Agent0 Episode 10 avgR=-1.0000\n",
      " \n",
      " Agent0 Episode 20 avgR=-0.9998\n",
      " \n",
      " Agent0 Episode 30 avgR=-0.9996\n",
      " \n",
      " Agent0 Episode 40 avgR=-0.9998\n",
      " \n",
      " Agent0 Episode 50 avgR=-0.9998\n",
      " \n",
      " Agent0 Episode 60 avgR=-1.0000\n",
      " \n",
      "  âœ“ Policy parameters stabilized (change < 0.001)\n",
      "  ðŸŽ¯ AGENT 0 CONVERGED! ðŸŽ¯\n",
      "  Policy criteria satisfied\n",
      "âœ“ Agent 0 converged after 63 episodes\n",
      "\n",
      "--- Training Agent 1 ---\n",
      " Agent1 Episode 10 avgR=0.9998\n",
      " \n",
      "  âœ“ Policy parameters stabilized (change < 0.001)\n",
      "  ðŸŽ¯ AGENT 1 CONVERGED! ðŸŽ¯\n",
      "  Policy criteria satisfied\n",
      "âœ“ Agent 1 converged after 13 episodes\n",
      "1   \u001b[92m0\u001b[0m    \n",
      "  #     #\n",
      "    #    \n",
      "\u001b[94m1\u001b[0m     # #\n",
      "  # 0    \n",
      "----------\n",
      "    \u001b[92m0\u001b[0m    \n",
      "1 #     #\n",
      "    #    \n",
      "\u001b[94m1\u001b[0m     # #\n",
      "  # 0    \n",
      "----------\n",
      "    \u001b[92m0\u001b[0m    \n",
      "  #     #\n",
      "1   #    \n",
      "\u001b[94m1\u001b[0m     # #\n",
      "  # 0    \n",
      "----------\n",
      "    \u001b[92m0\u001b[0m    \n",
      "  #     #\n",
      "    #    \n",
      "1     # #\n",
      "  # 0    \n",
      "----------\n",
      "\n",
      "--- Round 17 Policy Comparison ---\n",
      "\n",
      "  Comparing Agent 0 policies (Round 16 vs Round 17):\n",
      "    Policy parameter change: 0.032742 (threshold: 0.001)\n",
      "    ðŸ”„ Agent 0 policies still changing between rounds\n",
      "\n",
      "  Comparing Agent 1 policies (Round 16 vs Round 17):\n",
      "    Policy parameter change: 0.006524 (threshold: 0.001)\n",
      "    ðŸ”„ Agent 1 policies still changing between rounds\n",
      "\n",
      "ðŸ”„ 0/2 agents stabilized between rounds\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "=== Round 18 ===\n",
      "==================================================\n",
      "\n",
      "--- Training Agent 0 ---\n",
      " Agent0 Episode 10 avgR=-1.0000\n",
      " \n",
      "  âœ“ Policy parameters stabilized (change < 0.001)\n",
      "  ðŸŽ¯ AGENT 0 CONVERGED! ðŸŽ¯\n",
      "  Policy criteria satisfied\n",
      "âœ“ Agent 0 converged after 13 episodes\n",
      "\n",
      "--- Training Agent 1 ---\n",
      " Agent1 Episode 10 avgR=0.9998\n",
      " \n",
      "  âœ“ Policy parameters stabilized (change < 0.001)\n",
      "  ðŸŽ¯ AGENT 1 CONVERGED! ðŸŽ¯\n",
      "  Policy criteria satisfied\n",
      "âœ“ Agent 1 converged after 13 episodes\n",
      "1   \u001b[92m0\u001b[0m    \n",
      "  #     #\n",
      "    #    \n",
      "\u001b[94m1\u001b[0m     # #\n",
      "  # 0    \n",
      "----------\n",
      "    \u001b[92m0\u001b[0m    \n",
      "1 #     #\n",
      "    #    \n",
      "\u001b[94m1\u001b[0m     # #\n",
      "  # 0    \n",
      "----------\n",
      "    \u001b[92m0\u001b[0m    \n",
      "  #     #\n",
      "1   #    \n",
      "\u001b[94m1\u001b[0m     # #\n",
      "  # 0    \n",
      "----------\n",
      "    \u001b[92m0\u001b[0m    \n",
      "  #     #\n",
      "    #    \n",
      "1     # #\n",
      "  # 0    \n",
      "----------\n",
      "\n",
      "--- Round 18 Policy Comparison ---\n",
      "\n",
      "  Comparing Agent 0 policies (Round 17 vs Round 18):\n",
      "    Policy parameter change: 0.001942 (threshold: 0.001)\n",
      "    ðŸ”„ Agent 0 policies still changing between rounds\n",
      "\n",
      "  Comparing Agent 1 policies (Round 17 vs Round 18):\n",
      "    Policy parameter change: 0.006927 (threshold: 0.001)\n",
      "    ðŸ”„ Agent 1 policies still changing between rounds\n",
      "\n",
      "ðŸ”„ 0/2 agents stabilized between rounds\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "=== Round 19 ===\n",
      "==================================================\n",
      "\n",
      "--- Training Agent 0 ---\n",
      " Agent0 Episode 10 avgR=-1.0000\n",
      " \n",
      "  âœ“ Policy parameters stabilized (change < 0.001)\n",
      "  ðŸŽ¯ AGENT 0 CONVERGED! ðŸŽ¯\n",
      "  Policy criteria satisfied\n",
      "âœ“ Agent 0 converged after 13 episodes\n",
      "\n",
      "--- Training Agent 1 ---\n",
      " Agent1 Episode 10 avgR=1.0000\n",
      " \n",
      "  âœ“ Policy parameters stabilized (change < 0.001)\n",
      "  ðŸŽ¯ AGENT 1 CONVERGED! ðŸŽ¯\n",
      "  Policy criteria satisfied\n",
      "âœ“ Agent 1 converged after 13 episodes\n",
      "1   \u001b[92m0\u001b[0m    \n",
      "  #     #\n",
      "    #    \n",
      "\u001b[94m1\u001b[0m     # #\n",
      "  # 0    \n",
      "----------\n",
      "    \u001b[92m0\u001b[0m    \n",
      "1 #     #\n",
      "    #    \n",
      "\u001b[94m1\u001b[0m     # #\n",
      "  # 0    \n",
      "----------\n",
      "    \u001b[92m0\u001b[0m    \n",
      "  #     #\n",
      "1   #    \n",
      "\u001b[94m1\u001b[0m     # #\n",
      "  # 0    \n",
      "----------\n",
      "    \u001b[92m0\u001b[0m    \n",
      "  #     #\n",
      "    #    \n",
      "1     # #\n",
      "  # 0    \n",
      "----------\n",
      "\n",
      "--- Round 19 Policy Comparison ---\n",
      "\n",
      "  Comparing Agent 0 policies (Round 18 vs Round 19):\n",
      "    Policy parameter change: 0.001662 (threshold: 0.001)\n",
      "    ðŸ”„ Agent 0 policies still changing between rounds\n",
      "\n",
      "  Comparing Agent 1 policies (Round 18 vs Round 19):\n",
      "    Policy parameter change: 0.006449 (threshold: 0.001)\n",
      "    ðŸ”„ Agent 1 policies still changing between rounds\n",
      "\n",
      "ðŸ”„ 0/2 agents stabilized between rounds\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "=== Round 20 ===\n",
      "==================================================\n",
      "\n",
      "--- Training Agent 0 ---\n",
      " Agent0 Episode 10 avgR=-0.9998\n",
      " \n",
      " Agent0 Episode 20 avgR=-0.9994\n",
      " \n",
      " Agent0 Episode 30 avgR=-1.0000\n",
      " \n",
      " Agent0 Episode 40 avgR=-0.9998\n",
      " \n",
      " Agent0 Episode 50 avgR=-1.0000\n",
      " \n",
      "  âœ“ Policy parameters stabilized (change < 0.001)\n",
      "  ðŸŽ¯ AGENT 0 CONVERGED! ðŸŽ¯\n",
      "  Policy criteria satisfied\n",
      "âœ“ Agent 0 converged after 54 episodes\n",
      "\n",
      "--- Training Agent 1 ---\n"
     ]
    }
   ],
   "source": [
    "# Initialize env with 2 agents\n",
    "\n",
    "#env = random_maze_env(size=(5,5), n_agents=2, n_walls=6, seed=554669014)   (1) 0.001 15k yes  | (2) 0.01 20k yes  30k yes  | (2) 0.005 20k yes  30k yes | (3) 0.001 15k yes\n",
    "#env = random_maze_env(size=(5,5), n_agents=2, n_walls=6, seed=403475465)   (1) 0.001 15k no   | (2) 0.01 20k yes  30k yes  | (2) 0.005 20k yes  30k yes | (3) 0.0001 15k no \n",
    "#env = random_maze_env(size=(5,5), n_agents=2, n_walls=6, seed=972745715)   (1) 0.001 15k yes  | (2) 0.01 20k no  30k yes   | (2) 0.005 20k no  30k yes  | (3) 0.0001 15k no\n",
    "#env = random_maze_env(size=(5,5), n_agents=2, n_walls=6, seed=302632242)   (1) -     15k no   | (2) 0.01 20k yes  30k no   | (2) 0.005 20k yes  30k no  | (3) 0.0001 15k no\n",
    "#env = random_maze_env(size=(5,5), n_agents=2, n_walls=6, seed=219122341)   (1) 0.001 15k yes  | - | (2) 0.005 20k yes  30k yes\n",
    "#env = random_maze_env(size=(5,5), n_agents=2, n_walls=6, seed=871220494)   (1) 0.001 15k no   | - | -\n",
    "#env = random_maze_env(size=(5,5), n_agents=2, n_walls=6,seed=47399376)     (1) 0.001 15k yes  | -  | -\n",
    "#env = random_maze_env(size=(5,5), n_agents=2, n_walls=6, seed=83129399)    (1) 0.001 15k yes  | -  | (2) 0.005 20k yes  30k yes\n",
    "#env = random_maze_env(size=(5,5), n_agents=2, n_walls=6, seed=938621412)   (1) 0.001 15k yes  | -  | -\n",
    "#env = random_maze_env(size=(5,5), n_agents=2, n_walls=6, seed=86246299)    (1) 0.001 15k yes  | -  | -\n",
    "#env = random_maze_env(size=(5,5), n_agents=2, n_walls=6, seed=163628333)\n",
    "env = random_maze_env(size=(5,5), n_agents=2, n_walls=6, seed=584698030)\n",
    "env.render()\n",
    "\n",
    "# Two policies + optimizers\n",
    "policies = [PolicyNet(state_size=2) for _ in range(env.n_agents)]\n",
    "optimizers = [optim.Adam(p.parameters(), lr=0.01) for p in policies]\n",
    "\n",
    "scores = reinforce_multi_rwd2go_alt(env, policies, optimizers)\n",
    "scores = np.array(scores)\n",
    "\n",
    "# Two policies + optimizers\n",
    "#policies = [LinearPolicy(state_size=2) for _ in range(env.n_agents)]\n",
    "#optimizers = [optim.Adam(p.parameters(), lr=0.0001) for p in policies]\n",
    "\n",
    "#scores = reinforce_multi_rwd2go_seq_stop(env, policies, optimizers)\n",
    "#scores = np.array(scores)\n",
    "#solution1(env, scores, policies) \n",
    "\n",
    "# Two policies + optimizers\n",
    "#policies = [LinearPolicy(state_size=2) for _ in range(env.n_agents)]\n",
    "#optimizers = [optim.Adam(p.parameters(), lr=0.0001) for p in policies]\n",
    "\n",
    "#scores = reinforce_multi_rwd2go_seq(env, policies, optimizers,  n_episodes=15000)\n",
    "#scores = np.array(scores)\n",
    "#solution1(env, scores, policies) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad796189-6ad7-4e83-a7ef-b8b996ce0ac5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL_Project",
   "language": "python",
   "name": "rl_project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
